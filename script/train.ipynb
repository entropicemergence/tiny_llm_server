{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b8972-afc0-4253-9414-87833463bc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import tokenizer_basic\n",
    "import transformer_tiny\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c1b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fa724e-63a0-400f-87d3-ab9760d5a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "dataset_url = \"https://github.com/entropicemergence/tiny_llm_server/releases/download/v0.1.0/TinyStoriesV2-GPT4-small.zip\"\n",
    "# Storing dataset in a 'dataset' folder at the project root level.\n",
    "dataset_dir = os.path.join(\"..\", \"dataset\")\n",
    "file_name = dataset_url.split('/')[-1]\n",
    "zip_file_path = os.path.join(dataset_dir, file_name)\n",
    "\n",
    "# --- Ensure dataset directory exists ---\n",
    "os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "# --- Download dataset if it doesn't exist ---\n",
    "if not os.path.exists(zip_file_path):\n",
    "    print(f\"Dataset not found locally. Downloading from {dataset_url}...\")\n",
    "    try:\n",
    "        response = requests.get(dataset_url)\n",
    "        response.raise_for_status()\n",
    "        with open(zip_file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Dataset downloaded and saved to {zip_file_path}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the dataset: {e}\")\n",
    "else:\n",
    "    print(f\"Using existing dataset from {zip_file_path}\")\n",
    "\n",
    "# --- Load stories from the local zip file ---\n",
    "tiny_stories = []\n",
    "try:\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "        file_list = z.namelist()\n",
    "        print(f\"Files in zip: {file_list}\")\n",
    "\n",
    "        # Try to find the validation file, as was used in the original code\n",
    "        target_file_name = next((name for name in file_list if 'valid.txt' in name), None)\n",
    "\n",
    "        # Fallback to the first txt file if no validation file is found\n",
    "        if not target_file_name:\n",
    "            target_file_name = next((name for name in file_list if name.endswith('.txt')), None)\n",
    "\n",
    "        if target_file_name:\n",
    "            print(f\"Processing file: {target_file_name}\")\n",
    "            with z.open(target_file_name) as f:\n",
    "                with io.TextIOWrapper(f, encoding='utf-8') as file:\n",
    "                    story = \"\"\n",
    "                    for line in file:\n",
    "                        if line.strip() != \"<|endoftext|>\":\n",
    "                            story += line\n",
    "                        else:\n",
    "                            if story.strip():\n",
    "                                tiny_stories.append(story)\n",
    "                            story = \"\"\n",
    "                    if story.strip(): # Add the last story\n",
    "                        tiny_stories.append(story.strip())\n",
    "        else:\n",
    "            print(\"No suitable .txt file found in the zip archive.\")\n",
    "            \n",
    "    print(f\"Loaded {len(tiny_stories)} stories.\")\n",
    "    if len(tiny_stories) > 0:\n",
    "        print(\"Here are two random stories:\")\n",
    "        for _ in range(2):\n",
    "            print(\"---\")\n",
    "            print(tiny_stories[random.randint(0, len(tiny_stories) - 1)])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file {zip_file_path} could not be found. Please check the path or try downloading again.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing the zip file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7fa75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the tokenizer\n",
    "tokenizer = tokenizer_basic.HybridTokenizer(vocab_size=3200)\n",
    "\n",
    "\n",
    "# Build or load vocabulary from the loaded stories\n",
    "tokenizer.build_vocab(tiny_stories)\n",
    "# tokenizer.load_vocab(\"tinystories_tokenizer_vocab.json\")\n",
    "\n",
    "print(f\"\\nTokenizer trained successfully!\")\n",
    "print(f\"Total vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "print(f\"Word vocabulary: {len(tokenizer.word_to_id)} tokens\")\n",
    "print(f\"Character vocabulary: {len(tokenizer.char_to_id)} characters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2709fd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer with sample stories\n",
    "print(\"=== TOKENIZER TESTING ===\\n\")\n",
    "\n",
    "# Test with a sample story\n",
    "test_story = tiny_stories[0]  # First 200 chars of first story\n",
    "print(f\"Original text: {test_story}\")\n",
    "print()\n",
    "\n",
    "# Encode the story\n",
    "encoded = tokenizer.encode(test_story)\n",
    "print(f\"Encoded IDs: {encoded}\")\n",
    "print(f\"Sequence length: {len(encoded)}\")\n",
    "print()\n",
    "\n",
    "# Decode back to text\n",
    "decoded = tokenizer.decode(encoded)\n",
    "print(f\"Decoded text: {decoded}\")\n",
    "print()\n",
    "\n",
    "# Test with a sentence containing rare words\n",
    "test_rare = \"The xylophone played melodious symphonies while extraordinary creatures danced.\"\n",
    "print(f\"Test with rare words: {test_rare}\")\n",
    "encoded_rare = tokenizer.encode(test_rare)\n",
    "print(f\"Encoded: {encoded_rare}\")\n",
    "decoded_rare = tokenizer.decode(encoded_rare)\n",
    "print(f\"Decoded: {decoded_rare}\")\n",
    "print()\n",
    "\n",
    "# Show some examples of word vs character encoding\n",
    "print(\"=== WORD vs CHARACTER ENCODING EXAMPLES ===\")\n",
    "test_words = [\"the\", \"and\", \"was\", \"xylophone\", \"extraordinary\", \"symphonies\"]\n",
    "for word in test_words:\n",
    "    encoded_word = tokenizer.encode_word_or_chars(word)\n",
    "    is_word_level = len(encoded_word) == 1 and encoded_word[0] < len(tokenizer.word_to_id)\n",
    "    encoding_type = \"WORD\" if is_word_level else \"CHAR\"\n",
    "    print(f\"'{word}' -> {encoded_word} ({encoding_type})\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Test batch encoding for PyTorch\n",
    "print(\"=== PYTORCH BATCH ENCODING ===\")\n",
    "test_batch = [\n",
    "    \"Once upon a time, there was a little girl.\",\n",
    "    \"She loved to play with her toys.\",\n",
    "    \"The end.\"\n",
    "]\n",
    "\n",
    "batch_encoded = tokenizer.encode_batch(test_batch, max_length=20, padding=True)\n",
    "print(f\"Batch input_ids shape: {batch_encoded['input_ids'].shape}\")\n",
    "print(f\"Batch attention_mask shape: {batch_encoded['attention_mask'].shape}\")\n",
    "print(f\"First sequence: {batch_encoded['input_ids'][0].tolist()}\")\n",
    "print(f\"First attention mask: {batch_encoded['attention_mask'][0].tolist()}\")\n",
    "\n",
    "# Show vocabulary statistics\n",
    "print(f\"\\n=== VOCABULARY STATISTICS ===\")\n",
    "print(f\"Most frequent words (top 10):\")\n",
    "word_items = [(word, idx) for word, idx in tokenizer.word_to_id.items() \n",
    "              if word not in tokenizer.special_tokens]\n",
    "word_items_sorted = sorted(word_items, key=lambda x: x[1])[:10]\n",
    "for word, idx in word_items_sorted:\n",
    "    print(f\"  {idx}: '{word}'\")\n",
    "\n",
    "print(f\"\\nSpecial tokens:\")\n",
    "for token, idx in tokenizer.special_tokens.items():\n",
    "    print(f\"  {idx}: {token}\")\n",
    "\n",
    "print(f\"\\nCharacter vocabulary (first 20):\")\n",
    "char_items = sorted(tokenizer.char_to_id.items(), key=lambda x: x[1])[:20]\n",
    "for char, idx in char_items:\n",
    "    display_char = repr(char) if char in ['\\n', '\\t', ' '] else char\n",
    "    print(f\"  {idx + len(tokenizer.word_to_id)}: {display_char}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c697f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer vocabulary\n",
    "tokenizer.save_vocab(\"tinystories_tokenizer_vocab.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03685ee-8279-42fb-97ac-dca6451d48bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53687692-bac9-41fd-aa15-0a4d9ed598f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example: Create a PyTorch Dataset class for training\n",
    "class TinyStoriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, stories, tokenizer, max_length=512):\n",
    "        self.stories = stories\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.stories)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        story = self.stories[idx]\n",
    "        \n",
    "        # Encode the story\n",
    "        encoded = self.tokenizer.encode(story, add_special_tokens=True)\n",
    "        \n",
    "        # Truncate if too long\n",
    "        if len(encoded) > self.max_length:\n",
    "            encoded = encoded[:self.max_length]\n",
    "        \n",
    "        # For language modeling, input is sequence[:-1] and target is sequence[1:]\n",
    "        input_ids = encoded[:-1] if len(encoded) > 1 else [self.tokenizer.pad_id]\n",
    "        labels = encoded[1:] if len(encoded) > 1 else [self.tokenizer.pad_id]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "dataset = TinyStoriesDataset(tiny_stories, tokenizer, max_length=512)  # First 1000 stories\n",
    "print(f\"Dataset created with {len(dataset)} stories\")\n",
    "# print(dataset[0])\n",
    "\n",
    "# Test dataset\n",
    "sample = dataset[0]\n",
    "print(f\"Sample input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Sample labels shape: {sample['labels'].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Pad sequences in batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Pad to max length in batch\n",
    "    max_len = max(len(seq) for seq in input_ids)\n",
    "    \n",
    "    padded_inputs = []\n",
    "    padded_labels = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for i in range(len(input_ids)):\n",
    "        seq_len = len(input_ids[i])\n",
    "        \n",
    "        # Pad input_ids\n",
    "        padded_input = torch.cat([\n",
    "            input_ids[i],\n",
    "            torch.full((max_len - seq_len,), tokenizer.pad_id, dtype=torch.long)\n",
    "        ])\n",
    "        \n",
    "        # Pad labels  \n",
    "        padded_label = torch.cat([\n",
    "            labels[i],\n",
    "            torch.full((max_len - seq_len,), -100, dtype=torch.long)  # -100 is ignored in loss\n",
    "        ])\n",
    "        \n",
    "        # Create attention mask\n",
    "        attention_mask = torch.cat([\n",
    "            torch.ones(seq_len, dtype=torch.long),\n",
    "            torch.zeros(max_len - seq_len, dtype=torch.long)\n",
    "        ])\n",
    "        \n",
    "        padded_inputs.append(padded_input)\n",
    "        padded_labels.append(padded_label)\n",
    "        attention_masks.append(attention_mask)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': torch.stack(padded_inputs),\n",
    "        'labels': torch.stack(padded_labels),\n",
    "        'attention_mask': torch.stack(attention_masks)\n",
    "    }\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Test batch\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"input_ids: {batch['input_ids'].shape}\")\n",
    "print(f\"labels: {batch['labels'].shape}\")\n",
    "print(f\"attention_mask: {batch['attention_mask'].shape}\")\n",
    "\n",
    "print(f\"\\n=== TOKENIZER READY FOR TRAINING ===\")\n",
    "print(f\"Vocabulary size: {tokenizer.get_vocab_size()}\")\n",
    "print(f\"Word-level tokens: {len(tokenizer.word_to_id)}\")\n",
    "print(f\"Character-level tokens: {len(tokenizer.char_to_id)}\")\n",
    "print(f\"Dataset ready with {len(dataset)} samples\")\n",
    "print(f\"DataLoader ready with batch size 4\")\n",
    "\n",
    "# Example of loading the tokenizer later\n",
    "print(f\"\\n=== EXAMPLE: LOADING SAVED TOKENIZER ===\")\n",
    "new_tokenizer = tokenizer_basic.HybridTokenizer()\n",
    "new_tokenizer.load_vocab(\"tinystories_tokenizer_vocab.json\")\n",
    "print(f\"Loaded tokenizer with vocab size: {new_tokenizer.get_vocab_size()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf1b1c-138a-499a-b7c4-bffeff216b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_transformer=transformer_tiny.TinyTransformer(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    n_embd=192,\n",
    "    n_head=6,\n",
    "    n_layer=6,\n",
    "    max_context=512,\n",
    "    dropout=0.1\n",
    ")\n",
    "print(tiny_transformer)\n",
    "total_params = sum(p.numel() for p in tiny_transformer.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2ad49",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(\"final_model.pth\")\n",
    "tiny_transformer.load_state_dict(torch.load(\"final_model.pth\")['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a987e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f41183-526a-4ee9-9d28-6d71e7076c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "num_epochs = 3\n",
    "learning_rate = 3e-4\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "tiny_transformer = tiny_transformer.to(device)\n",
    "\n",
    "# Setup optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(tiny_transformer.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)  # -100 tokens are ignored\n",
    "\n",
    "# Training statistics\n",
    "train_losses = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(f\"Total batches per epoch: {len(dataloader)}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    tiny_transformer.train()\n",
    "    epoch_losses = []\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    # Progress bar for the epoch\n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', \n",
    "                leave=True, dynamic_ncols=True)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        # Move batch to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get model outputs (logits)\n",
    "        outputs = tiny_transformer(input_ids)  # Shape: [batch_size, seq_len, vocab_size]\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        # outputs: [batch_size * seq_len, vocab_size]\n",
    "        # labels: [batch_size * seq_len]\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), labels.reshape(-1))\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(tiny_transformer.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track loss\n",
    "        current_loss = loss.item()\n",
    "        epoch_losses.append(current_loss)\n",
    "        train_losses.append(current_loss)\n",
    "        \n",
    "        # Update progress bar\n",
    "        avg_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f'{current_loss:.4f}',\n",
    "            'Avg Loss': f'{avg_loss:.4f}',\n",
    "            'LR': f'{learning_rate:.2e}'\n",
    "        })\n",
    "        \n",
    "        # Removed frequent batch logging to keep progress bar clean\n",
    "        # All relevant info (Loss, Avg Loss, LR) is shown in the progress bar\n",
    "    \n",
    "    # End of epoch statistics\n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs} Summary:\")\n",
    "    print(f\"  Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"  Time: {epoch_time:.2f}s\")\n",
    "    print(f\"  Batches processed: {len(epoch_losses)}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_epoch_loss < best_loss:\n",
    "        best_loss = avg_epoch_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': tiny_transformer.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "            'vocab_size': tokenizer.get_vocab_size(),\n",
    "        }, 'best_model.pth')\n",
    "        print(f\"  ✓ New best model saved! Loss: {best_loss:.4f}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"\\n🎉 Training completed!\")\n",
    "print(f\"Final average loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Best loss achieved: {best_loss:.4f}\")\n",
    "print(f\"Total training samples processed: {len(train_losses)}\")\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': tiny_transformer.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': train_losses[-1],\n",
    "    'vocab_size': tokenizer.get_vocab_size(),\n",
    "    'train_losses': train_losses,\n",
    "}, 'final_model2.pth')\n",
    "print(\"Final model saved as 'final_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4fa682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(train_losses) > 0:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot 1: Raw loss over batches\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, alpha=0.7, linewidth=0.8)\n",
    "    plt.title('Training Loss Over Batches')\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Smoothed loss (moving average)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    window_size = min(50, len(train_losses) // 10)  # Adaptive window size\n",
    "    if window_size > 1:\n",
    "        smoothed_loss = np.convolve(train_losses, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(range(window_size-1, len(train_losses)), smoothed_loss, color='red', linewidth=2)\n",
    "        plt.title(f'Smoothed Training Loss (window={window_size})')\n",
    "    else:\n",
    "        plt.plot(train_losses, color='red', linewidth=2)\n",
    "        plt.title('Training Loss')\n",
    "    \n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_progress.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Training statistics:\")\n",
    "    print(f\"  Initial loss: {train_losses[0]:.4f}\")\n",
    "    print(f\"  Final loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  Best loss: {best_loss:.4f}\")\n",
    "    print(f\"  Loss improvement: {train_losses[0] - train_losses[-1]:.4f}\")\n",
    "    print(f\"  Relative improvement: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.2f}%\")\n",
    "else:\n",
    "    print(\"No training losses to plot. Run the training cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa9beb6-281e-4b97-bf42-789943e94d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"this morning i walk\"\n",
    "max_token = 128\n",
    "\n",
    "# Set model to evaluation mode\n",
    "tiny_transformer.eval()\n",
    "\n",
    "# Encode the prompt\n",
    "prompt_ids = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Encoded prompt: {prompt_ids}\")\n",
    "\n",
    "# Convert to tensor and add batch dimension\n",
    "input_ids = torch.tensor(prompt_ids, dtype=torch.long).unsqueeze(0)  # Shape: [1, seq_len]\n",
    "\n",
    "# Move to device if using GPU\n",
    "device = next(tiny_transformer.parameters()).device\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "print(f\"Generating {max_token} new tokens...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Generation parameters\n",
    "temperature = 0.0\n",
    "top_k = 50\n",
    "\n",
    "# Keep track of generated tokens\n",
    "generated_tokens = []\n",
    "current_sequence = input_ids.clone()\n",
    "\n",
    "print(\"Streaming generation:\")\n",
    "print(f\"Starting with: '{tokenizer.decode(current_sequence[0].tolist())}'\")\n",
    "print(\"\\nGenerated tokens:\")\n",
    "\n",
    "# Generate tokens one by one\n",
    "with torch.no_grad():\n",
    "    for step in range(max_token):\n",
    "        # Crop sequence to max context if needed\n",
    "        if current_sequence.size(1) > tiny_transformer.max_context:\n",
    "            current_sequence = current_sequence[:, -tiny_transformer.max_context:]\n",
    "        \n",
    "        # Forward pass to get logits\n",
    "        logits = tiny_transformer(current_sequence)  # Shape: [1, seq_len, vocab_size]\n",
    "        \n",
    "        # Get logits for the last position (next token prediction)\n",
    "        next_token_logits = logits[:, -1, :]  # Shape: [1, vocab_size]\n",
    "        \n",
    "        # Handle different sampling strategies\n",
    "        if temperature == 0.0:\n",
    "            # Greedy decoding: choose token with highest logit\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)  # Shape: [1, 1]\n",
    "        else:\n",
    "            # Temperature sampling\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            # Apply top-k filtering\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))\n",
    "                next_token_logits[next_token_logits < v[:, [-1]]] = -float('inf')\n",
    "            \n",
    "            # Convert logits to probabilities\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample next token\n",
    "            next_token = torch.multinomial(probs, num_samples=1)  # Shape: [1, 1]\n",
    "        \n",
    "        # Add to sequence\n",
    "        current_sequence = torch.cat([current_sequence, next_token], dim=1)\n",
    "        \n",
    "        # Store the generated token\n",
    "        token_id = next_token[0, 0].item()\n",
    "        generated_tokens.append(token_id)\n",
    "        \n",
    "        # Decode and print the new token\n",
    "        try:\n",
    "            # Try to decode just this token\n",
    "            if token_id in tokenizer.id_to_word:\n",
    "                token_text = tokenizer.id_to_word[token_id]\n",
    "            else:\n",
    "                # For character-level tokens, decode in context\n",
    "                token_text = tokenizer.decode([token_id])\n",
    "            \n",
    "            print(f\"Step {step+1:3d}: Token ID {token_id:4d} -> '{token_text}'\")\n",
    "            \n",
    "            # Check for end of sequence\n",
    "            if token_id == tokenizer.eos_id:\n",
    "                print(\"End of sequence token generated!\")\n",
    "                break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Step {step+1:3d}: Token ID {token_id:4d} -> [decode error: {e}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Generation complete!\")\n",
    "\n",
    "# Decode the full generated sequence\n",
    "full_sequence = current_sequence[0].tolist()\n",
    "full_text = tokenizer.decode(full_sequence)\n",
    "print(f\"Full generated text: '{full_text}'\")\n",
    "\n",
    "# Show just the new part\n",
    "original_length = len(prompt_ids)\n",
    "new_tokens_text = tokenizer.decode(generated_tokens)\n",
    "print(f\"Original prompt: '{prompt}'\")\n",
    "print(f\"Generated continuation: '{new_tokens_text}'\")\n",
    "print(f\"Total tokens generated: {len(generated_tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c9c3ae-02d8-4f38-923b-208590de3cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814fd91c-0cbb-4f80-bc9e-6059fb756e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "export_dir = '../model/weights'\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "state_dict = tiny_transformer.state_dict()\n",
    "\n",
    "with open(os.path.join(export_dir, 'metadata.txt'), 'w') as f:\n",
    "    for name, tensor in state_dict.items():\n",
    "        np_array = tensor.detach().cpu().numpy().astype(np.float32)\n",
    "        # print (np_array[0][:10])\n",
    "        bin_path = os.path.join(export_dir, name.replace('.', '_') + '.bin')\n",
    "        np_array.tofile(bin_path)\n",
    "        \n",
    "        shape_str = ' '.join(map(str, tensor.shape))\n",
    "        f.write(f\"{name} {shape_str} float32 {np_array.size}\\n\")\n",
    "\n",
    "print(f\"Weights exported to '{export_dir}' with metadata.txt for pure C++ loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c89d3-f07b-4953-80fe-1d97f32624c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d987a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b81b218",
   "metadata": {},
   "source": [
    "# Model Compute Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b05875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook System for tiny_transformer - exactly like the example\n",
    "prompt = \"this morning i walk\"\n",
    "\n",
    "# Set model to evaluation mode\n",
    "tiny_transformer.eval()\n",
    "\n",
    "# Encode the prompt\n",
    "prompt_ids = tokenizer.encode(prompt, add_special_tokens=True)\n",
    "\n",
    "# Convert to tensor and add batch dimension\n",
    "input_ids = torch.tensor(prompt_ids, dtype=torch.long).unsqueeze(0)  # Shape: [1, seq_len]\n",
    "\n",
    "# Move to device if using GPU\n",
    "device = next(tiny_transformer.parameters()).device\n",
    "input_ids = input_ids.to(device)\n",
    "\n",
    "# Define a dictionary to store the outputs\n",
    "activations = {}\n",
    "\n",
    "# Define a hook function\n",
    "def get_hook(name):\n",
    "    def hook_fn(module, input, output):\n",
    "        activations[name] = output\n",
    "    return hook_fn\n",
    "\n",
    "# Register hooks for all layers\n",
    "hooks = []\n",
    "for name, layer in tiny_transformer.named_modules():\n",
    "    hooks.append(layer.register_forward_hook(get_hook(name)))\n",
    "\n",
    "print(f\"Registered {len(hooks)} hooks on all layers\")\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "\n",
    "# Run the forward pass\n",
    "tiny_transformer.eval()\n",
    "with torch.no_grad():\n",
    "    out = tiny_transformer(input_ids)\n",
    "\n",
    "print(f\"Model output shape: {out.shape}\")\n",
    "print(f\"Model output sum: {torch.sum(out)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LAYER ACTIVATIONS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Access the stored outputs\n",
    "for name, activation in activations.items():\n",
    "    try:\n",
    "        if isinstance(activation, torch.Tensor):\n",
    "            print(f\"Layer: {name:40s} | Shape: {str(activation.shape):20s} | Sum: {torch.sum(activation.to(dtype=torch.float64)):15.10f} | Norm: {torch.norm(activation.to(dtype=torch.float64)):15.10f}\")\n",
    "        else:\n",
    "            print(f\"Layer: {name:40s} | Type: {type(activation).__name__:20s} | Content: {str(activation)[:50]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Layer: {name:40s} | Error: {str(e)[:50]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"Total layers with activations captured: {len([a for a in activations.values() if isinstance(a, torch.Tensor)])}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Remove the hooks\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "    \n",
    "print(f\"All {len(hooks)} hooks removed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in tiny_transformer.named_parameters():\n",
    "    print(f\"{name}: {param.data[0,:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd3d9dd-b430-4604-bc59-1ea423b38b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc6f762",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec071b-655f-4d83-a114-abdc99606f91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babeb307-2b5d-428d-af5d-201120386671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b6106-3b05-4b34-91b0-4e65a542a7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578704f2-001f-411b-8a00-626e615412a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94573253-3c7d-45c4-9a02-8d7888682bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72b2fb-ad2d-495b-a4f6-74b02b5b4863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
